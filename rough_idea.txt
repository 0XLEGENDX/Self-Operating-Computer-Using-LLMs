28-02-2025


**Project Proposal: Self-Operating Computer Using Large Language Models (LLMs)**

### **Objective**
The goal of this project is to develop a fully autonomous computing system powered by advanced Large Language Models (LLMs). Unlike existing AI agents that specialize in controlling browsers or specific tools with hardcoded instructions, this system will dynamically interact with the operating environment, enabling true automation without explicit programming for each new task.

### **Example Features**
1. **Autonomous Web Interaction**  
   - Open and navigate web browsers.
   - Search for information and extract relevant data.
   - Perform complex web-based operations dynamically.

2. **Software Installation & Execution**  
   - Install software packages (e.g., JDK for Java development).
   - Set up development environments.
   - Compile, execute, and debug Java projects automatically.

3. **Dynamic Problem Solving**  
   - Detect and resolve software-related issues autonomously.
   - Provide detailed debugging insights and suggest fixes.
   - Adapt to new environments without explicit reprogramming.

### **Technical Approach**
- **Vision-Enabled LLMs**: Incorporating Vision LLMs to interpret screen content, enabling real-time interaction with graphical user interfaces.
- **Multi-Modal AI Integration**: Combining natural language processing with computer vision and automation frameworks.
- **Reinforcement Learning**: Implementing a feedback loop to improve decision-making based on past interactions.

### **Challenges & Future Scope**
- Ensuring system stability and security while granting extensive system permissions.
- Optimizing real-time performance for seamless user experience.
- Expanding the systemâ€™s adaptability to various operating systems and applications.

This project is open to contributions, and collaboration is encouraged to overcome challenges and push the boundaries of autonomous computing.






01-03-2025



LLMs cannot give us pinpoint location to click for mouse or typing data.
This leads to one challenge of identifying buttons and other normal components in the window.
For ex there are multiple components in an application like buttons, images, icons, clickable icons, text, etc.
Challenge is to identify each component with accuracy and map them so vision llm can tell where to click or submit data.



To solve this we'll use ocr and image detector based approach where we'll read all the text using ocr and find all the icons through
some object detection algorithm. This will help us find all the buttons and text. After doing this we will simply label icons with some numbers
to the buttons and tell the vision model to return the button number or text which we have to click. This appraoch is not tested but is possible.
In case of multiple same text buttons, we'll again prompt by labelling these buttons.





Remember to remove google-genai

